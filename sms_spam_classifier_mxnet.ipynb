{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SMS Spam Classifier</h1>\n",
    "<br />\n",
    "This notebook shows how to implement a basic spam classifier for SMS messages using Apache MXNet as deep learning framework.\n",
    "The idea is to use the SMS spam collection dataset available at <a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\">https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</a> to train and deploy a neural network model by leveraging on the built-in open-source container for Apache MXNet available in Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by setting some configuration variables and getting the Amazon SageMaker session and the current execution role, using the Amazon SageMaker high-level SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::353077126427:role/service-role/AmazonSageMaker-ExecutionRole-20211211T112698\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "bucket_name = 'smlambda-workshop-rrr'\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_key_prefix = 'sms-spam-classifier'\n",
    "vocabulary_length = 9013\n",
    "\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now download the spam collection dataset, unzip it and read the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  198k  100  198k    0     0   309k      0 --:--:-- --:--:-- --:--:--  309k\n",
      "Archive:  dataset/smsspamcollection.zip\n",
      "  inflating: dataset/SMSSpamCollection  \n",
      "  inflating: dataset/readme          \n",
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "ham\tU dun say so early hor... U c already then say...\n",
      "ham\tNah I don't think he goes to usf, he lives around here though\n",
      "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dataset\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip -o dataset/smsspamcollection.zip\n",
    "!unzip -o dataset/smsspamcollection.zip -d dataset\n",
    "!head -10 dataset/SMSSpamCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the dataset into a Pandas dataframe and execute some data preparation.\n",
    "More specifically we have to:\n",
    "<ul>\n",
    "    <li>replace the target column values (ham/spam) with numeric values (0/1)</li>\n",
    "    <li>tokenize the sms messages and encode based on word counts</li>\n",
    "    <li>split into train and test sets</li>\n",
    "    <li>upload to a S3 bucket for training</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sms_spam_classifier_utilities import one_hot_encode\n",
    "from sms_spam_classifier_utilities import vectorize_sequences\n",
    "\n",
    "df = pd.read_csv('dataset/SMSSpamCollection', sep='\\t', header=None)\n",
    "df[df.columns[0]] = df[df.columns[0]].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "targets = df[df.columns[0]].values\n",
    "messages = df[df.columns[1]].values\n",
    "\n",
    "# one hot encoding for each SMS message\n",
    "one_hot_data = one_hot_encode(messages, vocabulary_length)\n",
    "encoded_messages = vectorize_sequences(one_hot_data, vocabulary_length)\n",
    "\n",
    "df2 = pd.DataFrame(encoded_messages)\n",
    "df2.insert(0, 'spam', targets)\n",
    "\n",
    "# Split into training and validation sets (80%/20% split)\n",
    "split_index = int(np.ceil(df.shape[0] * 0.8))\n",
    "train_set = df2[:split_index]\n",
    "val_set = df2[split_index:]\n",
    "\n",
    "train_set.to_csv('dataset/sms_train_set.gz', header=False, index=False, compression='gzip')\n",
    "val_set.to_csv('dataset/sms_val_set.gz', header=False, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to upload the two files back to Amazon S3 in order to be accessed by the Amazon SageMaker training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "with open('dataset/sms_train_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/train/sms_train_set.gz'.format(bucket_key_prefix))\n",
    "    \n",
    "with open('dataset/sms_val_set.gz', 'rb') as data:\n",
    "    target_bucket.upload_fileobj(data, '{0}/val/sms_val_set.gz'.format(bucket_key_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the model with MXNet</h2>\n",
    "\n",
    "We are now ready to run the training using the Amazon SageMaker MXNet built-in container. First let's have a look at the script defining our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "import pip\r\n",
      "\r\n",
      "try:\r\n",
      "    from pip import main as pipmain\r\n",
      "except:\r\n",
      "    from pip._internal import main as pipmain\r\n",
      "\r\n",
      "pipmain(['install', 'pandas'])\r\n",
      "import pandas\r\n",
      "\r\n",
      "#logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "          num_gpus, num_cpus, hosts, current_host, **kwargs):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters and apply some defaults in case they are not provided.\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "\r\n",
      "    train_data_path = channel_input_dirs['train']\r\n",
      "    val_data_path = channel_input_dirs['val']\r\n",
      "    train_data = get_train_data(train_data_path, batch_size)\r\n",
      "    val_data = get_val_data(val_data_path, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Normal(sigma=1.), ctx=ctx)\r\n",
      "    \r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    \r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        \r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            \r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "            L.backward()\r\n",
      "\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "\r\n",
      "            # update metric at last.\r\n",
      "            sigmoid_output = output.sigmoid() \r\n",
      "            prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "            metric.update([label], [prediction])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(64, activation=\"relu\"))\r\n",
      "        net.add(nn.Dense(1))\r\n",
      "    return net\r\n",
      "\r\n",
      "def get_train_data(data_path, batch_size):\r\n",
      "    print('Train data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_train_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=True)\r\n",
      "\r\n",
      "def get_val_data(data_path, batch_size):\r\n",
      "    print('Validation data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_val_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        \r\n",
      "        output = net(data)\r\n",
      "        sigmoid_output = output.sigmoid() \r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        metric.update([label], [prediction])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    net = gluon.nn.SymbolBlock(\r\n",
      "        outputs=mx.sym.load('%s/model.json' % model_dir),\r\n",
      "        inputs=mx.sym.var('data'))\r\n",
      "    \r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    try:\r\n",
      "        parsed = json.loads(data)\r\n",
      "        nda = mx.nd.array(parsed)\r\n",
      "        \r\n",
      "        output = net(nda)\r\n",
      "        sigmoid_output = output.sigmoid()\r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        output_obj = {}\r\n",
      "        output_obj['predicted_label'] = prediction.asnumpy().tolist()\r\n",
      "        output_obj['predicted_probability'] = sigmoid_output.asnumpy().tolist()\r\n",
      "\r\n",
      "        response_body = json.dumps(output_obj)\r\n",
      "        return response_body, output_content_type\r\n",
      "    except Exception as ex:\r\n",
      "        response_body = '{error: }' + str(ex)\r\n",
      "        return response_body, output_content_type\r\n",
      "    "
     ]
    }
   ],
   "source": [
    "!cat 'sms_spam_classifier_mxnet_script.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the training using the MXNet estimator object of the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-11 16:40:46 Starting - Starting the training job...\n",
      "2021-12-11 16:40:57 Starting - Launching requested ML instancesProfilerReport-1639240846: InProgress\n",
      "......\n",
      "2021-12-11 16:42:06 Starting - Preparing the instances for training.........\n",
      "2021-12-11 16:43:39 Downloading - Downloading input data\n",
      "2021-12-11 16:43:39 Training - Downloading the training image..\u001b[34m2021-12-11 16:43:50,960 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2021-12-11 16:43:50,960 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2021-12-11 16:43:50,964 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34m2021-12-11 16:43:51,848 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[34m2021-12-11 16:43:51,855 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'user_script_name': 'sms_spam_classifier_mxnet_script.py', 'input_config_dir': '/opt/ml/input/config', 'hyperparameters': {'sagemaker_program': 'sms_spam_classifier_mxnet_script.py', 'learning_rate': 0.01, 'epochs': 20, 'sagemaker_container_log_level': 20, 'sagemaker_region': 'us-east-1', 'sagemaker_job_name': 'sms-spam-classifier-mxnet-2021-12-11-16-40-46-049', 'batch_size': 100, 'sagemaker_submit_directory': 's3://smlambda-workshop-rrr/sms-spam-classifier/code/sms-spam-classifier-mxnet-2021-12-11-16-40-46-049/source/sourcedir.tar.gz'}, 'user_script_archive': 's3://smlambda-workshop-rrr/sms-spam-classifier/code/sms-spam-classifier-mxnet-2021-12-11-16-40-46-049/source/sourcedir.tar.gz', 'sagemaker_region': 'us-east-1', 'job_name': 'sms-spam-classifier-mxnet-2021-12-11-16-40-46-049', 'available_cpus': 8, '_scheduler_ip': '10.0.73.190', 'container_log_level': 20, 'resource_config': {'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}, '_ps_verbose': 0, 'model_dir': '/opt/ml/model', 'input_dir': '/opt/ml/input', '_scheduler_host': 'algo-1', 'code_dir': '/opt/ml/code', 'channel_dirs': {'val': '/opt/ml/input/data/val', 'train': '/opt/ml/input/data/train'}, 'available_gpus': 0, 'output_dir': '/opt/ml/output', 'output_data_dir': '/opt/ml/output/data/', 'hosts': ['algo-1'], 'channels': {'val': {'TrainingInputMode': 'File', 'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated'}, 'train': {'TrainingInputMode': 'File', 'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated'}}, 'enable_cloudwatch_metrics': False, 'current_host': 'algo-1', 'base_dir': '/opt/ml', '_ps_port': 8000, 'user_requirements_file': None}\u001b[0m\n",
      "\u001b[34mDownloading s3://smlambda-workshop-rrr/sms-spam-classifier/code/sms-spam-classifier-mxnet-2021-12-11-16-40-46-049/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2021-12-11 16:43:52,108 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[34mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/e3/d9f046b5d1c94a3aeab15f1f867aa414f8ee9d196fae6865f1d6a0ee1a0b/pytz-2021.3-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2021.3\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:190: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[34mTrain data path: /opt/ml/input/data/train\u001b[0m\n",
      "\n",
      "2021-12-11 16:43:59 Training - Training image download completed. Training in progress.\u001b[34mValidation data path: /opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34m[Epoch 0] Training: accuracy=0.731658\u001b[0m\n",
      "\u001b[34m[Epoch 0] Validation: accuracy=0.745732\u001b[0m\n",
      "\u001b[34m[Epoch 1] Training: accuracy=0.812879\u001b[0m\n",
      "\u001b[34m[Epoch 1] Validation: accuracy=0.831087\u001b[0m\n",
      "\u001b[34m[Epoch 2] Training: accuracy=0.848777\u001b[0m\n",
      "\u001b[34m[Epoch 2] Validation: accuracy=0.851752\u001b[0m\n",
      "\u001b[34m[Epoch 3] Training: accuracy=0.874804\u001b[0m\n",
      "\u001b[34m[Epoch 3] Validation: accuracy=0.868823\u001b[0m\n",
      "\u001b[34m[Epoch 4] Training: accuracy=0.884003\u001b[0m\n",
      "\u001b[34m[Epoch 4] Validation: accuracy=0.879605\u001b[0m\n",
      "\u001b[34m[Epoch 5] Training: accuracy=0.902176\u001b[0m\n",
      "\u001b[34m[Epoch 5] Validation: accuracy=0.893980\u001b[0m\n",
      "\u001b[34m[Epoch 6] Training: accuracy=0.904644\u001b[0m\n",
      "\u001b[34m[Epoch 6] Validation: accuracy=0.886792\u001b[0m\n",
      "\u001b[34m[Epoch 7] Training: accuracy=0.916985\u001b[0m\n",
      "\u001b[34m[Epoch 7] Validation: accuracy=0.893980\u001b[0m\n",
      "\u001b[34m[Epoch 8] Training: accuracy=0.919677\u001b[0m\n",
      "\u001b[34m[Epoch 8] Validation: accuracy=0.918239\u001b[0m\n",
      "\u001b[34m[Epoch 9] Training: accuracy=0.926632\u001b[0m\n",
      "\u001b[34m[Epoch 9] Validation: accuracy=0.920036\u001b[0m\n",
      "\u001b[34m[Epoch 10] Training: accuracy=0.930222\u001b[0m\n",
      "\u001b[34m[Epoch 10] Validation: accuracy=0.922731\u001b[0m\n",
      "\u001b[34m[Epoch 11] Training: accuracy=0.930671\u001b[0m\n",
      "\u001b[34m[Epoch 11] Validation: accuracy=0.921833\u001b[0m\n",
      "\u001b[34m[Epoch 12] Training: accuracy=0.933588\u001b[0m\n",
      "\u001b[34m[Epoch 12] Validation: accuracy=0.926325\u001b[0m\n",
      "\u001b[34m[Epoch 13] Training: accuracy=0.934709\u001b[0m\n",
      "\u001b[34m[Epoch 13] Validation: accuracy=0.923630\u001b[0m\n",
      "\u001b[34m[Epoch 14] Training: accuracy=0.937626\u001b[0m\n",
      "\u001b[34m[Epoch 14] Validation: accuracy=0.924528\u001b[0m\n",
      "\u001b[34m[Epoch 15] Training: accuracy=0.940992\u001b[0m\n",
      "\u001b[34m[Epoch 15] Validation: accuracy=0.934412\u001b[0m\n",
      "\u001b[34m[Epoch 16] Training: accuracy=0.939646\u001b[0m\n",
      "\u001b[34m[Epoch 16] Validation: accuracy=0.927224\u001b[0m\n",
      "\u001b[34m[Epoch 17] Training: accuracy=0.941216\u001b[0m\n",
      "\u001b[34m[Epoch 17] Validation: accuracy=0.936208\u001b[0m\n",
      "\u001b[34m[Epoch 18] Training: accuracy=0.942562\u001b[0m\n",
      "\u001b[34m[Epoch 18] Validation: accuracy=0.938005\u001b[0m\n",
      "\u001b[34m[Epoch 19] Training: accuracy=0.939421\u001b[0m\n",
      "\u001b[34m[Epoch 19] Validation: accuracy=0.928122\u001b[0m\n",
      "\n",
      "2021-12-11 16:44:45 Uploading - Uploading generated training model\n",
      "2021-12-11 16:44:45 Completed - Training job completed\n",
      "Training seconds: 81\n",
      "Billable seconds: 81\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "output_path = 's3://{0}/{1}/output'.format(bucket_name, bucket_key_prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, bucket_key_prefix)\n",
    "\n",
    "m = MXNet('sms_spam_classifier_mxnet_script.py',\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          train_instance_type='ml.c5.2xlarge',\n",
    "          output_path=output_path,\n",
    "          base_job_name='sms-spam-classifier-mxnet',\n",
    "          py_version=\"py3\",\n",
    "          framework_version=\"1.2\",\n",
    "          code_location = code_location,\n",
    "          hyperparameters={'batch_size': 100,\n",
    "                         'epochs': 20,\n",
    "                         'learning_rate': 0.01})\n",
    "\n",
    "inputs = {'train': 's3://{0}/{1}/train/'.format(bucket_name, bucket_key_prefix),\n",
    " 'val': 's3://{0}/{1}/val/'.format(bucket_name, bucket_key_prefix)}\n",
    "\n",
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:red\">THE FOLLOWING STEPS ARE NOT MANDATORY IF YOU PLAN TO DEPLOY TO AWS LAMBDA AND ARE INCLUDED IN THIS NOTEBOOK FOR EDUCATIONAL PURPOSES.</span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deploying the model</h2>\n",
    "\n",
    "Let's deploy the trained model to a real-time inference endpoint fully-managed by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "mxnet_pred = m.deploy(initial_instance_count=1,\n",
    "                      instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Executing Inferences</h2>\n",
    "\n",
    "Now, we can invoke the Amazon SageMaker real-time endpoint to execute some inferences, by providing SMS messages and getting the predicted label (SPAM = 1, HAM = 0) and the related probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.model import MXNetPredictor\n",
    "from sms_spam_classifier_utilities import one_hot_encode\n",
    "from sms_spam_classifier_utilities import vectorize_sequences\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "# mxnet_pred = MXNetPredictor('<endpoint_name>')\n",
    "\n",
    "test_messages = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\"]\n",
    "one_hot_test_messages = one_hot_encode(test_messages, vocabulary_length)\n",
    "encoded_test_messages = vectorize_sequences(one_hot_test_messages, vocabulary_length)\n",
    "\n",
    "result = mxnet_pred.predict(encoded_test_messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning-up</h2>\n",
    "\n",
    "When done, we can delete the Amazon SageMaker real-time inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_pred.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
